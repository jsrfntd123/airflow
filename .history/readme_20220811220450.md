AIRFLOW CONCEPTS:

* Open source platform to programmatically author,schedule and monitor workflows.
* Benefits:
    -Dynamic
    -Scalability
    -UI
    -Entenxibility
* Core components:
    -Web server: Flask Python Web server for user interface.
    -Scheduler: Schedule your jobs.
    -Metastore: Metadata about pipelines. Can be sqllite, postgres, mysql etc.
    -Triggerer: Allows execute specific kind of tasks.
    -Executor: How you execute the tasks. For examples kubernetes cluster, Celery Cluster (Run Python Taks in multiple machines). There are three types of executor: 
        - Sequential: It is the default executor. It only is able to run one task at time. It isnt able to run 
        - Local: Can run multiple tasks intance in paralell but in a single machine.
        - Celery: Run multiple tasks in different machines. In order to send the tasks to the workers celery implement a middleware called celery queue which have two components:
            - Broker: In which the scheduler put the task to be executed by the workers, The worker consume from this broker.
            - Result backend: Where the worker put the result of a specific task instance.
              The configuration parameters for celery are the following: 
                -"executor" (CeleryExecutor)
                -"sql_alchemy_conn" 
                -"celery_result_backend"
                -"celery_broker_url"
              You can access to the celery monitor server in : localhost:5555
        
        -K8s
    -Queue: Tasks are pushed in order.
    -Worker: Where your tasks  are executed.
    -Dag: 
    -Operator: Encansulapte what you want to do.
        -Action operator: Execute code.
        -Transfer operator: Tranfer data between different storage systems. Under the wood there are components called hooks wich abstract the complexity of the connection operations to different targets.
        -Sensor operator: 
            Waiting before moving the next tasks. This have two important concepts:
                - Poke time: The time sensor checkt the source
                - Timeout:
     For example you can have an operator for cleaning the data and other to process the data. It allow a low coupling.

* Airflow is not:
    -Datastreeaming solution o dataprocesing framework.
    -Not to scale data pipelines every second.
    -It can be used like a trigget to the tool that will process the data.
    -You can process data in ariflow but you can end up with overflow errors.

* How Airflow works.
    - Every five minutes the scheduler look for new dags in the DAGs folder.
    - The DAG is ready to be triggered.
    - The scheduler creates a DAG run object and create it with status "RUNNING".
    - The task instance is sent to the executor
    - The executor put the task in a set process or worker in oder to execute it.
    - As sson as it is done, the executor update the status of the task instnace object in the database.

Important concepts: 

* Airflow is an orchestrator, not a processing framework, process your gigabytes of data outside of Airflow (i.e. You have a Spark cluster, you use an operator to execute a Spark job, the data is processed in Spark).

* A DAG is a data pipeline, an Operator is a task.

* An Executor defines how your tasks are execute whereas a worker is a process executing your task

* The scheduler schedules your tasks, the web server serves the UI, the database stores the metadata of Airflow.

* airflow db init is the first command to execute to initialise Airflow

* If a task fails, check the logs by clicking on the task from the UI and "Logs"

* The Gantt view is super useful to sport bottlenecks and tasks are too long to execute


* Airflow Providers: They are connector to differents systems to add new functionalities and interactions.


AIRFLOW TESTING:
*  - docker exec -it airflow-scheduler /bin/bash
   - airflow tasks test user_processing create_table 2022-07-08

AIRFLOW CONFIGURATION FILES:
* The configuration files are in the following folder: /opt/airflow/

IMPOTANT PRAMETERS:  If you want to specify the following parameters at DAG level you only put the paramater in the definition of the DAG in code.
* parallelism: Total umber of tasks instaces that the ariflow instace can execute at same time.
* dag_concurrency: (concurrency in parameter code) Total umber of tasks instaces that the ariflow instace can execute at same time in one DAG (For example we have 10 instances for the same DAG, then if you set the parameter in 1 only one intances can execute a task and the other have to wait).
* max_active_runs_per_day: (max_active_run in parameter code) The maximum umber of active DAG runs per DAG.

TRANSFER DATA BEWEEN STAGES IN AIRFLOW
* External tool: Is a good option but add complexity to the pipeline.
* Xcom: Cross comuication. Allows to exchange small amount of data. Xcoms are limitated by size. It depends of airflow data store.
    * SQL Lite: 2GB
    * Postgres: 1GB
    * Mysql: 64 KB
* Airflow create an xcom parameter by defult if we add a return statement to the function called by the operator or in a explicit way throut accessing to task instance object. Example: ti.xcom_push(key='accuracy',value:123)
* For avoid create xcom parameters make the following configuration at the end of the function: do_xcom_push:False

TRIGGER RULES: There are 9 types:
* all_success: All the upstream task should be success to execute the following task.
* all_failed: The task is executed if all upstream tasks failed.
* all_done: The task is executed if all upstream tasks were executed and doesnt mind their state.
* one_success: If one of the upstream task have success the task is executed.
* one_failed: If one of the upstream task have failed the task is executed.
* none_failed: All the upstream tasks were succeded or skipped and for this reason the following task can be executed.
* none_failed_or_skipped: At least one parent has to be succed


PLUGINS:
* It is a way to create wour own operators.
* Plugins have lazy load.




ELASTIC SEARCH CONFIGURATION:
* Run into the folder elastic_search the command: docker-compose up
* Obtain the file /usr/share/elasticsearch/config/certs/ca/ca.crt from any of the nodes of the cluster. You an enter into the container wit this command: docker exec -u root -it elastic_search-kibana-1 bash
run ony of the following commands : 
    - curl --cacert ca.crt -XGET "https://localhost:9200" -u elastic:Sebitas123  to test. (Password was set in .env file in elastic_search folder)
    - curl -k -XGET "https://localhost:9200" -u elastic:Sebitas123  (Without authentication)
* You can disable de security options in the es cluster  setting the env variable in the docker compose file for each cluster now in this way: 
    - xpack.security.enabled=false

* To test a querie in elastic-search: curl -X GET "http://localhost:9200/connections/_search" -H "Content-type: application/json" -d '{"query":{"match_all":{}}}'


